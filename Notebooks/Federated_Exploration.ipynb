{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5100b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class SearchEnv(gym.Env):\n",
    "    _random_object_count = 48\n",
    "    \n",
    "    def __init__(self, map_size, vision_distance):\n",
    "        self._vision_distance = vision_distance\n",
    "        self._map_size = map_size\n",
    "        self._viewer = None\n",
    "        self._rendering_grid = [[None] * self._map_size for _ in range(self._map_size)]\n",
    "        \n",
    "        # Number of actions we can take (up, right, down, left, no_move) (5)\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "        \n",
    "        # Map\n",
    "        # 4 binary feature maps:\n",
    "        #   explored space\n",
    "        #   obstacles\n",
    "        #   observed robot positions\n",
    "        #   (goal candidates) not implemented yet\n",
    "        self._num_elements = self._map_size * self._map_size\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(3, self._map_size, self._map_size))\n",
    "        \n",
    "        # Reset the system\n",
    "        self.reset()\n",
    "                \n",
    "    def step(self, action):\n",
    "        # Move robot and update state\n",
    "        self._move_robot(action)\n",
    "            \n",
    "        reward = self._calculate_reward()\n",
    "\n",
    "        # explore in the square around us\n",
    "        self._update_observed_space()\n",
    "\n",
    "        # Check if fully explored\n",
    "        done = self._check_complete()\n",
    "        if done:\n",
    "            reward += 100\n",
    "\n",
    "        # set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        return np.array([self._exploration_map, self._object_map, self._robot_map]), reward, done, info\n",
    "    \n",
    "    def _move_robot(self, action):\n",
    "        # TODO: Robot cannot move into other robot or wall\n",
    "        x, y = self._robot_position\n",
    "        # Move up\n",
    "        if action == 0:\n",
    "            if (y > 0 and self._object_map[y-1][x] == 0):\n",
    "                self._robot_position = (x, y - 1)\n",
    "        \n",
    "        # Move right\n",
    "        elif action == 1:\n",
    "            if(x < self._map_size - 1 and self._object_map[y][x + 1] == 0):\n",
    "                self._robot_position = (x + 1, y)\n",
    "        \n",
    "        # Move down\n",
    "        elif action == 2:\n",
    "            if(y < self._map_size - 1 and self._object_map[y + 1][x] == 0):\n",
    "                self._robot_position = (x, y + 1)\n",
    "        \n",
    "        # Move left\n",
    "        elif action == 3:\n",
    "            if(x > 0  and self._object_map[y][x - 1] == 0):\n",
    "                self._robot_position = (x - 1, y)\n",
    "        \n",
    "        # no move as default/action 4\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def _calculate_reward(self):\n",
    "        prev_explored_cells = 0\n",
    "        unseen_cells = 0\n",
    "        x, y = self._robot_position\n",
    "        x_min = max(x - self._vision_distance, 0)\n",
    "        x_max = min(x + self._vision_distance, self._map_size - 1)\n",
    "        y_min = max(y - self._vision_distance, 0)\n",
    "        y_max = min(y + self._vision_distance, self._map_size - 1)\n",
    "        \n",
    "        for i in range(y_min, y_max + 1):\n",
    "            for j in range(x_min, x_max + 1):\n",
    "                # If we haven't seen this cell before\n",
    "                if self._exploration_map[i][j] == 0:\n",
    "                    unseen_cells += 1\n",
    "                else:\n",
    "                    prev_explored_cells += 1\n",
    "\n",
    "        return -1 + unseen_cells - prev_explored_cells\n",
    "\n",
    "    def _check_complete(self):\n",
    "        explored = np.count_nonzero(self._exploration_map)\n",
    "        return explored == self._num_elements\n",
    "\n",
    "    def _update_observed_space(self):\n",
    "        x, y = self._robot_position\n",
    "        x_min = max(x - self._vision_distance, 0)\n",
    "        x_max = min(x + self._vision_distance, self._map_size - 1)\n",
    "        y_min = max(y - self._vision_distance, 0)\n",
    "        y_max = min(y + self._vision_distance, self._map_size - 1)\n",
    "        \n",
    "        for i in range(y_min, y_max + 1):\n",
    "            for j in range(x_min, x_max + 1):\n",
    "                self._exploration_map[i][j] = 1\n",
    "                self._object_map[i][j] = self._master_object_map[i][j]\n",
    "                \n",
    "        # Update grid of robot positions\n",
    "        self._robot_map = [[0] * self._map_size for _ in range(self._map_size)]\n",
    "        self._robot_map[y][x] = 1\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        screen_size = 550\n",
    "        square_dimension = 0.0\n",
    "\n",
    "        if self._viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            \n",
    "            square_dimension = screen_size / self._map_size\n",
    "            self._viewer = rendering.Viewer(screen_size, screen_size)\n",
    "            \n",
    "            for i in range(self._map_size):\n",
    "                for j in range(self._map_size):\n",
    "                    l, r, t, b = (\n",
    "                        j * square_dimension,\n",
    "                        (j + 1) * square_dimension,\n",
    "                        i * square_dimension,\n",
    "                        (i + 1) * square_dimension,\n",
    "                    )\n",
    "                    square = rendering.FilledPolygon([(l, b), (l, t), (r, t), (r, b)])\n",
    "                    border = rendering.PolyLine([(l, b), (l, t), (r, t), (r, b)], True)\n",
    "\n",
    "                    self._rendering_grid[i][j] = square\n",
    "                    self._viewer.add_geom(square)\n",
    "                    self._viewer.add_geom(border)\n",
    "            \n",
    "        for i, row in enumerate(self._object_map):\n",
    "            for j, object_exists in enumerate(row):\n",
    "                square = self._rendering_grid[i][j]\n",
    "\n",
    "                # If a robot exists in this square\n",
    "                if (self._robot_map[i][j] == 1):\n",
    "                    #robot's pos\n",
    "                    square.set_color(0.8, 0.6, 0.4)\n",
    "\n",
    "                # if unexplored\n",
    "                elif (self._exploration_map[i][j] == 0):\n",
    "                    true_value = self._master_object_map[i][j]\n",
    "                    if (true_value == 0):\n",
    "                        square.set_color(0.8, 0.8, 0.8)\n",
    "                    else:\n",
    "                        square.set_color(0.3, 0.3, 0.3)\n",
    "\n",
    "                # Square is explored and blocked\n",
    "                elif (object_exists):\n",
    "                    square.set_color(0, 0, 0)\n",
    "\n",
    "                # square is explored and empty\n",
    "                else:\n",
    "                    square.set_color(1, 1, 1)\n",
    "\n",
    "        return self._viewer.render(return_rgb_array=(mode == \"rgb_array\"))\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset grid and positions\n",
    "        self._master_object_map = [[0] * self._map_size for _ in range(self._map_size)] # The env's reference for the world\n",
    "        self._exploration_map   = [[0] * self._map_size for _ in range(self._map_size)] # if a block has been explored\n",
    "        self._object_map        = [[0] * self._map_size for _ in range(self._map_size)] # if an object exists in the explored space\n",
    "        self._robot_map         = [[0] * self._map_size for _ in range(self._map_size)] # the last known positions of the robots\n",
    "\n",
    "        # Create new arena\n",
    "        self._create_arena()\n",
    "\n",
    "        # Put robot in random location that is empty (value == 0)\n",
    "        while True:\n",
    "            x, y = (\n",
    "                np.random.randint(self._map_size - 1), \n",
    "                np.random.randint(self._map_size - 1)\n",
    "            )\n",
    "            occupied = self._master_object_map[y][x]\n",
    "            # If unoccupied\n",
    "            if not occupied:\n",
    "                break\n",
    "            \n",
    "        self._robot_position = (x, y)\n",
    "        self._robot_map[y][x] = 1\n",
    "        \n",
    "        self._update_observed_space()\n",
    "\n",
    "        return np.array([self._exploration_map, self._object_map, self._robot_map])\n",
    "    \n",
    "    def _create_arena(self):\n",
    "        # Binary field where a 1 is an object, 0 is air\n",
    "        for _ in range(self._random_object_count):\n",
    "            box_size = np.random.randint(1, 3)\n",
    "            start_i = np.random.randint(1, self._map_size - box_size - 1)\n",
    "            start_j = np.random.randint(1, self._map_size - box_size - 1)\n",
    "            \n",
    "            for y in range(start_i, start_i + box_size + 1):\n",
    "                for x in range(start_j, start_j + box_size + 1):\n",
    "                    self._master_object_map[y][x] = 1\n",
    "    \n",
    "    def close(self):\n",
    "        if self._viewer:\n",
    "            self._viewer.close()\n",
    "            self._viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89982835",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SearchEnv(32, 2)\n",
    "states = env.observation_space.shape\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset()\n",
    "# done = False\n",
    "# score = 0\n",
    "\n",
    "# env.render()\n",
    "# n_state, reward, done, info = env.step(4)\n",
    "\n",
    "# while not done:\n",
    "#     env.render()\n",
    "#     action = env.action_space.sample()\n",
    "#     n_state, reward, done, info = env.step(action)\n",
    "#     score += reward\n",
    "#     print(n_state)\n",
    "#     input()\n",
    "        \n",
    "# # print(\"Score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a6199",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1b6b8",
   "metadata": {},
   "source": [
    "# Create DQN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00244d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f251bf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Flatten(input_shape=states))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f6e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = states\n",
    "model = build_model((1, x, y, z), actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c63d32",
   "metadata": {},
   "source": [
    "# Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad4a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=130000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, \n",
    "                   memory=memory, \n",
    "                   policy=policy, \n",
    "                   nb_actions=actions, \n",
    "                   nb_steps_warmup=150, \n",
    "                   target_model_update=1e-2)\n",
    "\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a34bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=100000, visualize=True, verbose=1)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "# plt.plot(avg_reward_list)\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c13dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.test(env, nb_episodes=3, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
